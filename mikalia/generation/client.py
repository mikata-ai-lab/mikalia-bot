"""
client.py â€” Wrapper del SDK de Anthropic para la Claude API.

Este archivo encapsula TODA la comunicaciÃ³n con Claude.

Â¿Por quÃ© un wrapper en vez de usar el SDK directamente?
    1. No repetir configuraciÃ³n en cada llamada (model, max_tokens, etc.)
    2. Manejar errores de API de forma centralizada
    3. Facilitar testing con mocks (solo mockeas MikaliaClient)
    4. Agregar retry automÃ¡tico con backoff exponencial
    5. Logging de cada llamada para debugging

Conceptos clave de la Anthropic API:
    - anthropic.Anthropic(): Crea el cliente principal
    - client.messages.create(): EnvÃ­a un mensaje a Claude
    - system: Instrucciones base (personalidad de Mikalia)
    - messages: Lista de mensajes usuario/asistente
    - max_tokens: LÃ­mite de la respuesta (4096 â‰ˆ ~3000 palabras)
    - temperature: Creatividad (0.0=determinista, 1.0=muy creativo)

Uso:
    from mikalia.generation.client import MikaliaClient
    client = MikaliaClient(api_key="sk-ant-...", personality=personality)
    respuesta = client.generate("Escribe sobre agentes de IA")
"""

from __future__ import annotations

import time
from dataclasses import dataclass, field
from typing import Any

import anthropic
from rich.console import Console

from mikalia.personality import Personality

# Consola de Rich para output bonito
console = Console()


@dataclass
class APIResponse:
    """
    Respuesta procesada de la Claude API.

    Attributes:
        content: Texto de la respuesta
        model: Modelo que generÃ³ la respuesta
        input_tokens: Tokens usados en el prompt
        output_tokens: Tokens generados en la respuesta
        stop_reason: RazÃ³n por la que Claude dejÃ³ de generar
    """
    content: str
    model: str
    input_tokens: int
    output_tokens: int
    stop_reason: str
    tool_calls: list[dict] = field(default_factory=list)
    raw_content: list[Any] = field(default_factory=list)

    @property
    def has_tool_use(self) -> bool:
        """True si Claude respondio con tool_use blocks."""
        return self.stop_reason == "tool_use" and len(self.tool_calls) > 0


class MikaliaClient:
    """
    Cliente wrapper para la Anthropic API.

    Encapsula la configuraciÃ³n y manejo de errores para que
    el resto del cÃ³digo solo necesite llamar generate() o review().

    Args:
        api_key: Clave de API de Anthropic (sk-ant-...)
        model: Modelo a usar (default: claude-sonnet-4-5-20250929)
        personality: Personalidad cargada (system prompt)
        max_retries: NÃºmero mÃ¡ximo de reintentos ante errores (default: 3)
    """

    def __init__(
        self,
        api_key: str,
        model: str = "claude-sonnet-4-5-20250929",
        personality: Personality | None = None,
        max_retries: int = 3,
    ):
        # Validar que tenemos API key
        if not api_key:
            raise ValueError(
                "No se proporcionÃ³ API key de Anthropic.\n"
                "Configura ANTHROPIC_API_KEY en tu archivo .env"
            )

        # Crear el cliente de Anthropic
        # El SDK se encarga de la conexiÃ³n HTTP, headers, etc.
        self._client = anthropic.Anthropic(api_key=api_key)
        self._model = model
        self._personality = personality
        self._max_retries = max_retries

        # System prompt: si hay personalidad, usarla; si no, string vacÃ­o
        self._system_prompt = personality.system_prompt if personality else ""

    def generate(
        self,
        user_prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 4096,
        system_override: str | None = None,
    ) -> APIResponse:
        """
        Genera contenido usando Claude API.

        Este es el mÃ©todo principal para CREAR contenido (posts, traducciones).
        Usa temperatura alta (0.7) para mÃ¡s creatividad.

        Args:
            user_prompt: Lo que le pedimos a Claude (el tema, instrucciones, etc.)
            temperature: Creatividad (0.7 para contenido, 0.3 para revisiÃ³n)
            max_tokens: LÃ­mite de la respuesta
            system_override: System prompt alternativo (ignora MIKALIA.md)

        Returns:
            APIResponse con el contenido generado y metadata.

        Ejemplo:
            respuesta = client.generate(
                "Escribe un post sobre agentes de IA en desarrollo de software"
            )
            print(respuesta.content)
        """
        # El system prompt puede ser el de MIKALIA.md o uno custom
        system = system_override or self._system_prompt

        return self._call_api(
            system=system,
            user_prompt=user_prompt,
            temperature=temperature,
            max_tokens=max_tokens,
        )

    def review(
        self,
        content: str,
        review_prompt: str,
        temperature: float = 0.3,
        max_tokens: int = 2048,
    ) -> APIResponse:
        """
        Revisa contenido usando Claude API con temperatura baja.

        Este mÃ©todo es para EVALUAR contenido (self-review).
        Usa temperatura baja (0.3) para ser mÃ¡s analÃ­tico y consistente.

        Args:
            content: El contenido a revisar
            review_prompt: Instrucciones de revisiÃ³n (criterios, formato)
            temperature: Creatividad baja para anÃ¡lisis (0.3 por default)
            max_tokens: LÃ­mite de la respuesta de revisiÃ³n

        Returns:
            APIResponse con el resultado de la revisiÃ³n.
        """
        # Para review, combinamos el contenido y las instrucciones
        prompt_completo = f"{review_prompt}\n\n---\n\n{content}"

        return self._call_api(
            system="Eres una editora exigente pero justa. Revisa contenido con ojo crÃ­tico.",
            user_prompt=prompt_completo,
            temperature=temperature,
            max_tokens=max_tokens,
        )

    def _call_api(
        self,
        system: str,
        user_prompt: str,
        temperature: float,
        max_tokens: int,
    ) -> APIResponse:
        """
        Hace la llamada real a la Anthropic API con retry automÃ¡tico.

        Â¿Por quÃ© retry con backoff exponencial?
        Porque las APIs pueden fallar temporalmente por:
        - Rate limiting (muchas llamadas muy rÃ¡pido)
        - Errores de servidor (500, 503)
        - Timeouts de red

        Backoff exponencial: espera 1s, luego 2s, luego 4s...
        Esto evita saturar la API con reintentos inmediatos.

        Args:
            system: System prompt (personalidad o instrucciones)
            user_prompt: Mensaje del usuario
            temperature: Creatividad
            max_tokens: LÃ­mite de respuesta

        Returns:
            APIResponse con el resultado.

        Raises:
            anthropic.APIError: Si todos los reintentos fallan.
        """
        ultimo_error = None

        for intento in range(self._max_retries):
            try:
                # La llamada real a Claude
                response = self._client.messages.create(
                    model=self._model,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    system=system,
                    messages=[
                        {"role": "user", "content": user_prompt}
                    ],
                )

                # Extraer el texto de la respuesta
                # Claude puede devolver mÃºltiples bloques de contenido,
                # pero para texto simple siempre es el primero
                texto = response.content[0].text

                return APIResponse(
                    content=texto,
                    model=response.model,
                    input_tokens=response.usage.input_tokens,
                    output_tokens=response.usage.output_tokens,
                    stop_reason=response.stop_reason,
                )

            except anthropic.RateLimitError as e:
                # Rate limiting: esperar mÃ¡s tiempo
                ultimo_error = e
                tiempo_espera = (2 ** intento) * 2  # 2s, 4s, 8s
                console.print(
                    f"[yellow]â³ Rate limit alcanzado. "
                    f"Esperando {tiempo_espera}s (intento {intento + 1}/{self._max_retries})...[/yellow]"
                )
                time.sleep(tiempo_espera)

            except anthropic.APIStatusError as e:
                # Errores de servidor (500, 503, etc.)
                ultimo_error = e
                if e.status_code >= 500:
                    tiempo_espera = 2 ** intento  # 1s, 2s, 4s
                    console.print(
                        f"[yellow]âš ï¸ Error del servidor ({e.status_code}). "
                        f"Reintentando en {tiempo_espera}s...[/yellow]"
                    )
                    time.sleep(tiempo_espera)
                else:
                    # Errores 4xx (bad request, auth, etc.) no se reintentan
                    raise

            except anthropic.APIConnectionError as e:
                # Error de red
                ultimo_error = e
                tiempo_espera = 2 ** intento
                console.print(
                    f"[yellow]ðŸŒ Error de conexiÃ³n. "
                    f"Reintentando en {tiempo_espera}s...[/yellow]"
                )
                time.sleep(tiempo_espera)

        # Si llegamos aquÃ­, todos los reintentos fallaron
        raise ultimo_error  # type: ignore[misc]

    def chat_with_tools(
        self,
        messages: list[dict],
        tools: list[dict] | None = None,
        system: str | None = None,
        temperature: float = 0.7,
        max_tokens: int = 4096,
    ) -> APIResponse:
        """
        Chat con soporte para tool_use de Claude.

        A diferencia de generate() que envia un solo prompt,
        este metodo maneja una sola ronda de Claude con tool definitions.
        El caller (MikaliaAgent) se encarga del loop de tool calls.

        Args:
            messages: Lista de mensajes en formato Claude API.
            tools: Definiciones de tools (de ToolRegistry).
            system: System prompt (override).
            temperature: Creatividad.
            max_tokens: Limite de respuesta.

        Returns:
            APIResponse con tool_calls y raw_content si Claude pidio tools.
        """
        system_prompt = system or self._system_prompt
        ultimo_error = None

        for intento in range(self._max_retries):
            try:
                kwargs: dict[str, Any] = {
                    "model": self._model,
                    "max_tokens": max_tokens,
                    "temperature": temperature,
                    "system": system_prompt,
                    "messages": messages,
                }
                if tools:
                    kwargs["tools"] = tools

                response = self._client.messages.create(**kwargs)

                # Extraer texto y tool_use blocks
                text_parts = []
                tool_calls = []
                raw_content = []

                for block in response.content:
                    if block.type == "text":
                        text_parts.append(block.text)
                        raw_content.append({
                            "type": "text",
                            "text": block.text,
                        })
                    elif block.type == "tool_use":
                        tool_calls.append({
                            "id": block.id,
                            "name": block.name,
                            "input": block.input,
                        })
                        raw_content.append({
                            "type": "tool_use",
                            "id": block.id,
                            "name": block.name,
                            "input": block.input,
                        })

                return APIResponse(
                    content="\n".join(text_parts),
                    model=response.model,
                    input_tokens=response.usage.input_tokens,
                    output_tokens=response.usage.output_tokens,
                    stop_reason=response.stop_reason,
                    tool_calls=tool_calls,
                    raw_content=raw_content,
                )

            except anthropic.RateLimitError as e:
                ultimo_error = e
                tiempo_espera = (2 ** intento) * 2
                time.sleep(tiempo_espera)

            except anthropic.APIStatusError as e:
                ultimo_error = e
                if e.status_code >= 500:
                    time.sleep(2 ** intento)
                else:
                    raise

            except anthropic.APIConnectionError as e:
                ultimo_error = e
                time.sleep(2 ** intento)

        raise ultimo_error  # type: ignore[misc]
